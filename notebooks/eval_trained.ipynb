{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4dc29770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# change to upper dir to use all custom libs (won't be needed if run from main scripts)\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4164845a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yadisk\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv(dotenv.find_dotenv())\n",
    "client = yadisk.Client(token=os.environ[\"YA_DISK_TOKEN\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f72bff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    }
   ],
   "source": [
    "from scripts.helpers.yadisk_manager import download_model\n",
    "\n",
    "\n",
    "experiments = {\n",
    "    # a minor problem with <2lang> token where language group was not passed correctly and initial token == <unk>\n",
    "#     'nllb.ru-mhr.both_direction.upd_tokenizer': \"/bs-diploma/experiments/kaggle/ru-mhr.both_direction.mari-parallel.nllb.upd_tokenizer.finally_fixed\",\n",
    "\n",
    "    'nllb.ru-mht.both_direction.old_tokenizer': '/bs-diploma/experiments/kaggle/ru-mhr.both_directions.mari-parallel.nllb.no_upd_tokenizer',\n",
    "    'nllb.ru-mhr.both_direction.upd_tokenizer': \"/bs-diploma/experiments/kaggle/ru-mhr.both_directions.mari-parallel-413k.nllb.no_upd_tokenizer.mhr_fixed\",\n",
    "    'nllb.ru-mhr.sing_direction.upd_tokenizer': \"/bs-diploma/experiments/kaggle/ru-mhr.single_direction.mari-parallel-413k.nllb.update_tokenizer\",\n",
    "    \n",
    "    \n",
    "    # TODO: rename to upd_tokenizer on ya.disk\n",
    "    'nllb.ru-tat.both_direction.upd_tokenizer': '/bs-diploma/experiments/kaggle/ru-tat.both_directions.ipsan.tatar-parallel-400k.nllb.upd_tokenizer',\n",
    "    'nllb.ru-tat.both_direction.old_tokenizer': '/bs-diploma/experiments/kaggle/ru-tt.both_direction.ipsan.nllb.base_tokenizer',\n",
    "#     'nllb.ru-tat.sing_direction.old_tokenizer': '',  # TODO: add\n",
    "#     'nllb.tat-ru.sing_direction.old_tokenizer': '',  # TODO: add\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "download_model(\n",
    "    client=client,\n",
    "    experiment=experiments['nllb.ru-mhr.both_direction.upd_tokenizer'],\n",
    "    folder='models',\n",
    "    checkpoint='last',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb97706f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cutefluffyfox\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    }
   ],
   "source": [
    "from scripts.helpers.model_manager import NLLB200Model\n",
    "\n",
    "model = NLLB200Model.from_folder(\n",
    "    model_path='models/model', \n",
    "    tokenizer_path='models/tokenizer', \n",
    "    convert_to_float16=True, \n",
    "    device='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a892278e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Здравствуйте!', 'Hello worlds!')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_text = model.translate_single('hello worlds!', 'ru', 'mhr')\n",
    "rus_text = model.translate_single(trn_text, 'mhr', 'ru')\n",
    "rus_text, trn_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "184c9d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " ... (more hidden) ...\n",
      " ... (more hidden) ...\n"
     ]
    }
   ],
   "source": [
    "from scripts.benchmark.flores_dev import benchmark as benchmark_flores_dev\n",
    "\n",
    "benchmark_flores_dev(model, 'ru-mhr')\n",
    "benchmark_flores_dev(model, 'mhr-ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "054c300a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using the latest cached version of the module from C:\\Users\\cutefluffyfox\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--bleu\\9e0985c1200e367cce45605ce0ecb5ede079894e0f24f54613fca08eeb8aff76 (last modified on Tue Nov  5 05:51:14 2024) since it couldn't be found locally at evaluate-metric--bleu, or remotely on the Hugging Face Hub.\n",
      "Using the latest cached version of the module from C:\\Users\\cutefluffyfox\\.cache\\huggingface\\modules\\evaluate_modules\\metrics\\evaluate-metric--chrf\\d244bab9383988714085a8dacc4871986d9f025398581c33d6b2ee22836b4069 (last modified on Tue Nov  5 05:54:58 2024) since it couldn't be found locally at evaluate-metric--chrf, or remotely on the Hugging Face Hub.\n"
     ]
    }
   ],
   "source": [
    "from scripts.helpers.metric_manager import BLEU, BLEURT, BertScore, CHRF, GoogleBLEU, FluencyRU\n",
    "\n",
    "metrics = [\n",
    "    BLEU(),\n",
    "#     BLEURT(),\n",
    "#     BertScore(),\n",
    "    CHRF(),\n",
    "#     FluencyRU(),\n",
    "#     GoogleBLEU()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ec8f586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "========== \\mhr-ru\\mhr_both_dir.token_upd.mhr_ru.last.txt ==========\n",
      "bleu : 0.1573674741106463\n",
      "chrf : 45.86165017933309\n",
      "========== \\mhr-ru\\mhr_both_dir.token_upd_broken.mhr_ru.last.txt ==========\n",
      "bleu : 0.08968501201433417\n",
      "chrf : 36.77403867823899\n",
      "========== \\mhr-ru\\mhr_both_dir.token_upd_broken.mhr_ru.7200.txt ==========\n",
      "bleu : 0.12787250706142736\n",
      "chrf : 43.42382341215098\n",
      "========== \\mhr-ru\\mhr_both_dir.old_tokenizer.mhr_ru.last.txt ==========\n",
      "bleu : 0.009983102617198474\n",
      "chrf : 16.366378658826093\n",
      "========== \\mhr-ru\\nllb200.mhr_ru.txt ==========\n",
      "bleu : 0.03707498445596439\n",
      "chrf : 27.610867511783933\n",
      "========== \\mhr-ru\\madlad400.mhr_ru.txt ==========\n",
      "bleu : 0.14416953753714376\n",
      "chrf : 38.20947588930588\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== \\ru-mhr\\mhr_both_dir.token_upd.ru_mhr.last.txt ==========\n",
      "bleu : 0.013422333820341265\n",
      "chrf : 14.586913117507091\n",
      "========== \\ru-mhr\\mhr_single_dir.token_upd.ru_mhr.last.txt ==========\n",
      "bleu : 0.0882602692724267\n",
      "chrf : 42.27905735258935\n",
      "========== \\ru-mhr\\mhr_both_dir.token_upd_broken.ru_mhr.last.txt ==========\n",
      "bleu : 0.07319940181732666\n",
      "chrf : 38.2987734504991\n",
      "========== \\ru-mhr\\mhr_both_dir.token_upd_broken.ru_mhr.7200.txt ==========\n",
      "bleu : 0.06997321836749203\n",
      "chrf : 39.04027280146239\n",
      "========== \\ru-mhr\\mhr_both_dir.old_tokenizer.ru_mhr.last.txt ==========\n",
      "bleu : 0.07523983105252627\n",
      "chrf : 39.58343756419703\n",
      "========== \\ru-mhr\\madlad400.ru_mhr.txt ==========\n",
      "bleu : 0.021880696091946265\n",
      "chrf : 19.71051192504408\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== \\tat-ru\\tat_both_dir.no_token_upd.tat_ru.last.txt ==========\n",
      "bleu : 0.12802585226874472\n",
      "chrf : 43.062998254126065\n",
      "========== \\tat-ru\\tat_both_dir.token_upd.tat_ru.last.txt ==========\n",
      "bleu : 0.17603458225325164\n",
      "chrf : 49.63806790229383\n",
      "========== \\tat-ru\\nllb200.tat_ru.txt ==========\n",
      "bleu : 0.16596088584576163\n",
      "chrf : 48.71357929052167\n",
      "========== \\tat-ru\\madlad400.tat_ru.txt ==========\n",
      "bleu : 0.07424180612979353\n",
      "chrf : 29.31793273469314\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== \\ru-tat\\tat_both_dir.no_token_upd.ru_tat.last.txt ==========\n",
      "bleu : 0.0025427718295185746\n",
      "chrf : 11.041698898840039\n",
      "========== \\ru-tat\\tat_both_dir.token_upd.ru_tat.last.txt ==========\n",
      "bleu : 0.02023876432426524\n",
      "chrf : 19.516940927963724\n",
      "========== \\ru-tat\\nllb200.ru_tat.txt ==========\n",
      "bleu : 0.0051465205977029175\n",
      "chrf : 2.0472916925776286\n",
      "========== \\ru-tat\\madlad400.ru_tat.txt ==========\n",
      "bleu : 0.0864821708840958\n",
      "chrf : 36.710466498175535\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== \\kaz-ru\\nllb200.kaz_ru.txt ==========\n",
      "bleu : 0.1519986654722386\n",
      "chrf : 47.59438904317666\n",
      "========== \\kaz-ru\\madlad400.kaz_ru.txt ==========\n",
      "bleu : 0.1802677874557238\n",
      "chrf : 46.330111649195125\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========== \\ru-kaz\\nllb200.ru_kaz.txt ==========\n",
      "bleu : 0.1279603371642969\n",
      "chrf : 49.1822706640367\n",
      "========== \\ru-kaz\\madlad400.ru_kaz.txt ==========\n",
      "bleu : 0.11336539762831352\n",
      "chrf : 43.9034846960344\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scripts.helpers.path_manager import DataManager\n",
    "\n",
    "\n",
    "\n",
    "dm = DataManager()\n",
    "translations = [\n",
    "    dm.get_path('benchmark', 'mhr-ru', 'mhr_both_dir.token_upd.mhr_ru.last.txt'),\n",
    "    dm.get_path('benchmark', 'mhr-ru', 'mhr_both_dir.token_upd_broken.mhr_ru.last.txt'),\n",
    "    dm.get_path('benchmark', 'mhr-ru', 'mhr_both_dir.token_upd_broken.mhr_ru.7200.txt'),\n",
    "    dm.get_path('benchmark', 'mhr-ru', 'mhr_both_dir.old_tokenizer.mhr_ru.last.txt'),\n",
    "    dm.get_path('benchmark', 'mhr-ru', 'nllb200.mhr_ru.txt'),\n",
    "    dm.get_path('benchmark', 'mhr-ru', 'madlad400.mhr_ru.txt'),\n",
    "\n",
    "    dm.get_path('benchmark', 'ru-mhr', 'mhr_both_dir.token_upd.ru_mhr.last.txt'),\n",
    "    dm.get_path('benchmark', 'ru-mhr', 'mhr_single_dir.token_upd.ru_mhr.last.txt'),\n",
    "    dm.get_path('benchmark', 'ru-mhr', 'mhr_both_dir.token_upd_broken.ru_mhr.last.txt'),\n",
    "    dm.get_path('benchmark', 'ru-mhr', 'mhr_both_dir.token_upd_broken.ru_mhr.7200.txt'),\n",
    "    dm.get_path('benchmark', 'ru-mhr', 'mhr_both_dir.old_tokenizer.ru_mhr.last.txt'),\n",
    "    dm.get_path('benchmark', 'ru-mhr', 'madlad400.ru_mhr.txt'),\n",
    "\n",
    "    \n",
    "    dm.get_path('benchmark', 'tat-ru', 'tat_both_dir.no_token_upd.tat_ru.last.txt'),\n",
    "    dm.get_path('benchmark', 'tat-ru', 'tat_both_dir.token_upd.tat_ru.last.txt'),\n",
    "    dm.get_path('benchmark', 'tat-ru', 'nllb200.tat_ru.txt'),\n",
    "    dm.get_path('benchmark', 'tat-ru', 'madlad400.tat_ru.txt'),\n",
    "\n",
    "    dm.get_path('benchmark', 'ru-tat', 'tat_both_dir.no_token_upd.ru_tat.last.txt'),\n",
    "    dm.get_path('benchmark', 'ru-tat', 'tat_both_dir.token_upd.ru_tat.last.txt'),\n",
    "    dm.get_path('benchmark', 'ru-tat', 'nllb200.ru_tat.txt'),\n",
    "    dm.get_path('benchmark', 'ru-tat', 'madlad400.ru_tat.txt'),\n",
    "\n",
    "    dm.get_path('benchmark', 'kaz-ru', 'nllb200.kaz_ru.txt'),\n",
    "    dm.get_path('benchmark', 'kaz-ru', 'madlad400.kaz_ru.txt'),\n",
    "\n",
    "    \n",
    "    dm.get_path('benchmark', 'ru-kaz', 'nllb200.ru_kaz.txt'),\n",
    "    dm.get_path('benchmark', 'ru-kaz', 'madlad400.ru_kaz.txt'),\n",
    "]\n",
    "\n",
    "\n",
    "prev_dir = ''\n",
    "for trans in translations:\n",
    "    direction = trans.split('benchmark')[-1].replace('\\\\', '/').strip('/').split('/')[0]\n",
    "    if prev_dir != direction:\n",
    "        print('\\n\\n\\n')\n",
    "        prev_dir = direction\n",
    "    \n",
    "    print('==========', trans.split('benchmark')[-1], '==========')\n",
    "    src_lang, dst_lang = direction.split('-')\n",
    "    for metric in metrics:\n",
    "        if not metric.lang_is_supported(metric):\n",
    "            continue\n",
    "        \n",
    "        with open(trans, 'r', encoding='UTF-8') as file:\n",
    "            res = [eval(line.strip()) for line in file.readlines()]\n",
    "        df = pd.DataFrame(res)\n",
    "        if dst_lang == 'ru' and 'ru' not in df.columns and 'rus' in df.columns:\n",
    "            dst_lang = 'rus'\n",
    "        res_trans, res_ref = df['translation'].to_list(), df[dst_lang].to_list()\n",
    "\n",
    "        print(metric.metric_name, ':', metric(\n",
    "            sources=None,\n",
    "            targets=res_ref,\n",
    "            translations=res_trans,\n",
    "            lang=dst_lang\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "092f172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:\\\\Users\\\\cutefluffyfox\\\\PycharmProjects\\\\MT-vocab-tuning\\\\data\\\\benchmark\\\\mhr-ru\\\\nllb200.mhr_ru.txt', 'r', encoding='UTF-8') as file:\n",
    "    print(eval(file.readlines()[942])['translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2842895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

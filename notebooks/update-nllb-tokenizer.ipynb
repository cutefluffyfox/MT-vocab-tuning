{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1aa51157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "# change to upper dir to use all custom libs (won't be needed if run from main scripts)\n",
    "sys.path.insert(1, os.path.join(sys.path[0], '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8460ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helpers.dataset_manager import HFDataset\n",
    "import pandas as pd\n",
    "\n",
    "# tt_ru = HFDataset('tat-ru', 'tatar-parallel-162k').get_df()\n",
    "mhr_ru = HFDataset('mhr-ru', 'mari-parallel-413k').get_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9a116fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 401446/401446 [00:26<00:00, 15072.17it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">3</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">from</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff; text-decoration: underline\">scripts.helpers.tokenizer_manager</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">import</span> add_new_full_language                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>3 model, tokenizer = add_new_full_language(data=mhr_ru, lang=<span style=\"color: #808000; text-decoration-color: #808000\">'mhr'</span>, similar_lang=<span style=\"color: #808000; text-decoration-color: #808000\">'udm'</span>)        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">4 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\cutefluffyfox\\PycharmProjects\\MT-vocab-tuning\\notebooks\\..\\scripts\\helpers\\tokenizer_ma</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">nager.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">174</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">add_new_full_language</span>                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">171 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>tokenizer = AutoTokenizer.from_pretrained(model_name, vocab_file=<span style=\"color: #808000; text-decoration-color: #808000\">f'nllb_{</span>lang<span style=\"color: #808000; text-decoration-color: #808000\">}'</span>)       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">172 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model = AutoModelForSeq2SeqLM.from_pretrained(model_name)                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">173 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># patching them</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>174 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>fix_tokenizer(tokenizer)                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">175 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>model.resize_token_embeddings(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(tokenizer))                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">176 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">177 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># fixing the new/moved token embeddings in the model</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">C:\\Users\\cutefluffyfox\\PycharmProjects\\MT-vocab-tuning\\notebooks\\..\\scripts\\helpers\\tokenizer_ma</span> <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #808000; text-decoration-color: #808000\">nager.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">142</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">fix_tokenizer</span>                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">139 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">Add a new language token to the tokenizer vocabulary</span>                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">140 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">(this should be done each time after its initialization)</span>                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">141 </span><span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">\"\"\"</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>142 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>old_len = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(tokenizer) - <span style=\"color: #00ffff; text-decoration-color: #00ffff\">int</span>(new_lang <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> tokenizer.added_tokens_encoder)             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">143 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>tokenizer.lang_code_to_id[new_lang] = old_len-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">144 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>tokenizer.id_to_lang_code[old_len-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>] = new_lang                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">145 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># always move \"mask\" to the last position</span>                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">AttributeError: </span><span style=\"color: #008000; text-decoration-color: #008000\">'NllbTokenizerFast'</span> object has no attribute <span style=\"color: #008000; text-decoration-color: #008000\">'added_tokens_encoder'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m3\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m1 \u001b[0m\u001b[94mfrom\u001b[0m \u001b[4;96mscripts\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mhelpers\u001b[0m\u001b[4;96m.\u001b[0m\u001b[4;96mtokenizer_manager\u001b[0m \u001b[94mimport\u001b[0m add_new_full_language                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m3 model, tokenizer = add_new_full_language(data=mhr_ru, lang=\u001b[33m'\u001b[0m\u001b[33mmhr\u001b[0m\u001b[33m'\u001b[0m, similar_lang=\u001b[33m'\u001b[0m\u001b[33mudm\u001b[0m\u001b[33m'\u001b[0m)        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m4 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\cutefluffyfox\\PycharmProjects\\MT-vocab-tuning\\notebooks\\..\\scripts\\helpers\\tokenizer_ma\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mnager.py\u001b[0m:\u001b[94m174\u001b[0m in \u001b[92madd_new_full_language\u001b[0m                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m171 \u001b[0m\u001b[2m│   \u001b[0mtokenizer = AutoTokenizer.from_pretrained(model_name, vocab_file=\u001b[33mf\u001b[0m\u001b[33m'\u001b[0m\u001b[33mnllb_\u001b[0m\u001b[33m{\u001b[0mlang\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m)       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m172 \u001b[0m\u001b[2m│   \u001b[0mmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name)                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m173 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# patching them\u001b[0m                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m174 \u001b[2m│   \u001b[0mfix_tokenizer(tokenizer)                                                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m175 \u001b[0m\u001b[2m│   \u001b[0mmodel.resize_token_embeddings(\u001b[96mlen\u001b[0m(tokenizer))                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m176 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m177 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# fixing the new/moved token embeddings in the model\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mC:\\Users\\cutefluffyfox\\PycharmProjects\\MT-vocab-tuning\\notebooks\\..\\scripts\\helpers\\tokenizer_ma\u001b[0m \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[33mnager.py\u001b[0m:\u001b[94m142\u001b[0m in \u001b[92mfix_tokenizer\u001b[0m                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m139 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33mAdd a new language token to the tokenizer vocabulary\u001b[0m                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m140 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m(this should be done each time after its initialization)\u001b[0m                               \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m141 \u001b[0m\u001b[2;33m│   \u001b[0m\u001b[33m\"\"\"\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m142 \u001b[2m│   \u001b[0mold_len = \u001b[96mlen\u001b[0m(tokenizer) - \u001b[96mint\u001b[0m(new_lang \u001b[95min\u001b[0m tokenizer.added_tokens_encoder)             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m143 \u001b[0m\u001b[2m│   \u001b[0mtokenizer.lang_code_to_id[new_lang] = old_len-\u001b[94m1\u001b[0m                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m144 \u001b[0m\u001b[2m│   \u001b[0mtokenizer.id_to_lang_code[old_len-\u001b[94m1\u001b[0m] = new_lang                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m145 \u001b[0m\u001b[2m│   \u001b[0m\u001b[2m# always move \"mask\" to the last position\u001b[0m                                              \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mAttributeError: \u001b[0m\u001b[32m'NllbTokenizerFast'\u001b[0m object has no attribute \u001b[32m'added_tokens_encoder'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scripts.helpers.tokenizer_manager import add_new_full_language\n",
    "\n",
    "model, tokenizer = add_new_full_language(data=mhr_ru, lang='mhr', similar_lang='udm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f06149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.helpers.model_manager import NLLB200Model\n",
    "\n",
    "nllb = NLLB200Model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945324c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c837ea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.helpers.tokenizer_manager import train_sentencepiece, preprocess_corpora\n",
    "\n",
    "# all_texts, required_chars = preprocess_corpora(\n",
    "#     data=mhr_ru,\n",
    "#     lang='mhr',\n",
    "# )\n",
    "\n",
    "# train_sentencepiece(\n",
    "#     all_texts=all_texts, \n",
    "#     required_chars=required_chars,\n",
    "#     tokenizer_prefix='smp_mahri_16k'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef358074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scripts.helpers.tokenizer_manager import merge_nllb_new_tokenizers\n",
    "# merge_nllb_new_tokenizers('smp_mahri_16k', 'nllb_mahri_268k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e1b92d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebad43b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ae649d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e47e956",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
